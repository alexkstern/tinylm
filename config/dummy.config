vocab_size: 50257
max_position_embeddings: 2048
n_layer: 8
n_head: 16
n_embd: 64
embed_dropout: 0.0
attention_dropout: 0.0
resid_dropout: 0.0
layer_norm_epsilon: 1e-5
initializer_range: 0.02
window_size: 256
local_heads: 4
trained_model_path: trained_models/test1
batch_size: 2
block_size: 2048
learning_rate: 5e-5
num_epochs: 3   
tokenizer_name: EleutherAI/gpt-neo-125M
use_data_fraction: 0.001
